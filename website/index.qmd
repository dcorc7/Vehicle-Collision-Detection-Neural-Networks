---
title: Predicting Vehicle Collisions from Dashcam Video Using Neural Network Models
---

# Introduction

In this project, we addressed the challenge of real-time collision detection and classification from dashcam videos, a task with significant implications for road safety and post-incident analysis. Our goal was to develop deep learning models capable of classifying video segments as either involving a collision or near miss, or no risk of collision at all. To achieve this, we extracted bounding box features from each frame of all provided dashcam videos using a YOLO object detection model. These raw bounding box and class features were then used to calculate additional attributes, such as changes in position and relative size over time, to better capture motion patterns and object interactions as frames progress through each video. Using these enhanced time-series representations, we evaluated the effectiveness of three sequence modeling architectures: Gated Recurrent Units (GRU), Long Short-Term Memory networks (LSTM), and Transformers. Each model was trained to recognize temporal patterns indicative of potential or ongoing collisions, supporting applications in real-time driver assistance systems and automated incident detection.

# Literature Review

Advancements in autonomous driving and intelligent safety systems have led to increased research in collision detection using computer vision and deep learning. Dashcam-based crash prediction is particularly relevant for real-time driver assistance, auto insurance evaluation, and autonomous navigation. This section reviews recent studies that integrate object detection and sequence modeling for collision analysis.

Nguyen et al. (2023) proposed a pipeline combining YOLOv5 and Mask R-CNN for object and lane detection in dashcam footage, tailored for auto insurance scenarios. Their hybrid approach improved detection accuracy in complex scenes through pre-training and fine-tuning on diverse driving conditions. Fernandes et al. (2018) developed a CNN-RNN system for road anomaly detection, emphasizing how sequential frame dependencies can enhance event recognition, laying groundwork for models like GRUs and LSTMs.
Saini et al. (2022) focused on fast, lightweight ML models for collision detection on embedded devices, achieving over 90% accuracy using sensor data and basic vision techniques. Though they avoided models like YOLO, their work underscores real-world constraints such as speed and resource limits.

Wee et al. (2022) presented a deep learning pipeline for forward collision warning, using YOLACT to balance detection accuracy and inference speed. Their system robustly handled challenging conditions like nighttime and rain, and their benchmark comparisons highlighted YOLACT as a practical compromise between performance and efficiency.
Across these works, a common theme emerges: combining robust object detection (e.g., YOLO, Mask R-CNN) with temporal or contextual modeling enhances collision prediction. Key challenges include occlusion handling, real-time processing, and robustness under diverse conditions. These insights directly shaped our approach, where YOLOv8 preprocessing feeds into sequential models for crash prediction.



The Github repository hosting all code for this project can be found [here](https://github.com/dcorc7/Vehicle-Collision-Detection-Neural-Networks)