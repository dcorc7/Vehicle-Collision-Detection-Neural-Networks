[
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "As seen in Figure 1, the LSTM model showed moderate performance, achieving a classification accuracy of approximately 66%. This means that just under two-thirds of the dashcam videos were correctly classified. With a precision of about 70.6%, the model was reasonably accurate when predicting crashes, which is beneficial in minimizing false alarms in safety-critical applications. However, its recall was lower at 51.4%, indicating that nearly half of the actual crash videos went undetected. This shortfall is significant, as failing to recognize real crashes poses serious risks in real-world deployment. The F1 score of 59.5% reflects the imbalance between precision and recall, suggesting that while the model avoids over-predicting crashes, it struggles to consistently detect them. Despite these limitations, the LSTM model demonstrates reasonable promise, but improving its sensitivity would be a necessary task for deployment in scenarios where missed detections can have significant consequences.\nThe training process involved monitoring both training and validation loss curves over multiple epochs to optimize the model. The best-performing model leveraged the following hyperparameters: a hidden size of 128, a dropout rate of 0.2, a learning rate of 0.001, three hidden layers, and the Adam optimizer.\n\n\n\n\n\nLSTM Confusion Matrix\n\n\n\n\n\n\nLSTM Loss Curves"
  },
  {
    "objectID": "results.html#limitations",
    "href": "results.html#limitations",
    "title": "Results",
    "section": "Limitations:",
    "text": "Limitations:\nAs reflected in our mostly moderate results, this study encountered several key challenges. One major limitation was visual obstruction in the dashcam footage caused by close-up objects, motion blur, poor camera angles, or climate-related distortions. These issues significantly impaired the performance of the pretrained YOLO object detection model, which ultimately reduced the quality of input data for the sequential neural networks. In some cases, even when there were no physical obstructions, the YOLO model misclassified objects, further degrading the input data quality. Consequently, the system occasionally failed to detect critical objects involved in actual collisions or mistakenly flagged non-threatening scenarios as crashes. Additionally, the majority of the collision videos featured only minor fender benders, meaning the visual cues for collision can be extremely subtle. This made it especially difficult for the model to distinguish low-impact incidents from regular traffic behaviors, such as a vehicle coming to a stop at a red light.\n\nExamples of Poor YOLO Detections and Dashcam Footage\n\n\n\nLSTM Loss Curves"
  },
  {
    "objectID": "results.html#future-directions",
    "href": "results.html#future-directions",
    "title": "Results",
    "section": "Future Directions:",
    "text": "Future Directions:\nTo address these limitations and push this work toward real-world deployment, several future directions are proposed:\n3D Motion Modeling: Integrating optical flow analysis, or estimating object velocities through pixel movement between frames, could provide better motion-based context beyond just features extracted from raw frames. This would hypothetically allow the model to better understand sudden movements and forceful impacts, both characteristic of collisions.\nMulti-Modal Sensors: Extending the model to incorporate additional sensor inputs, such as GPS, LiDAR, accelerometers, gyroscopes, or vehicle telemetry, could offer complementary information, improving classification under conditions considered ambiguous with just dashcam footage."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting Vehicle Collisions from Dashcam Video Using Neural Network Models",
    "section": "",
    "text": "Introduction\nIn this project, we addressed the challenge of real-time collision detection and classification from dashcam videos, a task with significant implications for road safety and post-incident analysis. Our goal was to develop deep learning models capable of classifying video segments as either involving a collision or near miss, or no risk of collision at all. To achieve this, we extracted bounding box features from each frame of all provided dashcam videos using a YOLO object detection model. These raw bounding box and class features were then used to calculate additional attributes, such as changes in position and relative size over time, to better capture motion patterns and object interactions as frames progress through each video. Using these enhanced time-series representations, we evaluated the effectiveness of three sequence modeling architectures: Gated Recurrent Units (GRU), Long Short-Term Memory networks (LSTM), and Transformers. Each model was trained to recognize temporal patterns indicative of potential or ongoing collisions, supporting applications in real-time driver assistance systems and automated incident detection.\n\n\nLiterature Review\nAdvancements in autonomous driving and intelligent safety systems have led to increased research in collision detection using computer vision and deep learning. Dashcam-based crash prediction is particularly relevant for real-time driver assistance, auto insurance evaluation, and autonomous navigation. This section reviews recent studies that integrate object detection and sequence modeling for collision analysis.\nNguyen et al. (2023) proposed a pipeline combining YOLOv5 and Mask R-CNN for object and lane detection in dashcam footage, tailored for auto insurance scenarios. Their hybrid approach improved detection accuracy in complex scenes through pre-training and fine-tuning on diverse driving conditions. Fernandes et al. (2018) developed a CNN-RNN system for road anomaly detection, emphasizing how sequential frame dependencies can enhance event recognition, laying groundwork for models like GRUs and LSTMs. Saini et al. (2022) focused on fast, lightweight ML models for collision detection on embedded devices, achieving over 90% accuracy using sensor data and basic vision techniques. Though they avoided models like YOLO, their work underscores real-world constraints such as speed and resource limits.\nWee et al. (2022) presented a deep learning pipeline for forward collision warning, using YOLACT to balance detection accuracy and inference speed. Their system robustly handled challenging conditions like nighttime and rain, and their benchmark comparisons highlighted YOLACT as a practical compromise between performance and efficiency. Across these works, a common theme emerges: combining robust object detection (e.g., YOLO, Mask R-CNN) with temporal or contextual modeling enhances collision prediction. Key challenges include occlusion handling, real-time processing, and robustness under diverse conditions. These insights directly shaped our approach, where YOLOv8 preprocessing feeds into sequential models for crash prediction.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "data_collection.html",
    "href": "data_collection.html",
    "title": "Data & Feature Engineering",
    "section": "",
    "text": "Dataset:\nThis study utilized 1,500 labeled dashcam videos sourced from the Nexar Dashcam Crash Prediction Challenge on Kaggle. Each video clip is roughly 40 seconds and categorized into one of two classes: collision/near miss or non-collision, with an equal distribution of 750 videos per class. The dataset reflects real-world driving conditions and includes a diverse range of environments and traffic scenarios, making it suitable for training temporal models for collision detection. The dataset was partitioned into training, validation, and testing sets using an 80/10/10 split. This ensures sufficient data for model training while reserving representative samples for hyperparameter tuning and final performance evaluation.\n\n\nPreprocessing:\nTo convert raw video data into a format suitable for sequence modeling, we performed several preprocessing steps. First, frames were extracted at a rate of 1 frame per second using OpenCV, resulting in a relatively consistent 40 frames per video. Each frame was then passed through a pre-trained YOLOv8 object detection model to identify and localize entities of interest at every time step/frame. The YOLOv8 model is configured to detect around 80 object classes, including many that would appear in dashcam footage, like cars, buses, trucks, pedestrians, and road signs. For each video, a structured “detections.csv” file was generated, logging detected objects and their characteristics. The raw features appearing in each “detections.csv” file include frame number, bounding box size variables (x_min, y_min, x_max, y_max), class, and confidence score. From these raw detections, we constructed and appended a suite of per-frame feature vectors by first aggregating and normalizing all of the object-level data over each frame. For each frame, we grouped detections by class and computed their summary statistics, including the count of classes, their mean & variance, and the mean area and aspect ratio of each class’s bounding box. Next, we linked objects across consecutive frames by constructing an IoU-based tracker to estimate each object’s centroid’s displacements, which we use as a proxy for speed and acceleration of different objects over time. To encode short-term temporal context, we also append delta-features (i.e. the frame-to-frame changes in object coins and motion metrics). Finally, all features were standardized using the global mean and standard deviation computed across the training set. All sequences include zero-padding to ensure a uniform length across inputs before feeding into our models.\n\n\nFinal Input Format:\nThe final model input consists of a sequence of feature vectors derived from the object detection outputs for each frame. Each sequence has a fixed length of 40 time steps (one per second), with features aggregated or encoded per frame. Sequences are padded and packed to ensure compatibility with PyTorch’s LSTM and GRU modules for batch processing. Each feature vector includes both raw object-level attributes and engineered temporal features that capture inter-frame changes, like shifts in position and relative size. To standardize the input across videos, we used StandardScaler from scikit-learn. This scaler was fit using all feature vectors extracted from the training set. For each video, features were computed frame by frame, and deltas were calculated for a subset of the features by comparing each frame with its preceding frame, while categorical or static features were excluded from delta computation. The final input representation for each frame combined the raw and delta features into a vector, which was then normalized using the fitted scaler before being passed to the models. The dimension of each final input vector varied, as feature importances were being experimented with. The input vectors contained 7, 16, and 28 features for the LSTM, GRU, and Transformer models, respectively.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "Methods",
    "section": "",
    "text": "We implemented three neural network architectures for binary classification: LSTM, GRU, and Transformer models. All three models are designed to capture temporal dependencies within video frame sequences by processing frame-level features sequentially. Each model takes a sequence of frame features as input, with dimensions corresponding to the number of frames (time steps) and feature size per frame.\n\nLong Short-Term Memory:\nLSTM networks are a type of recurrent neural network (RNN) designed to address the problem of vanishing gradients, which can occur when training standard RNNs over long sequences. LSTMs use gates (input, forget, and output) to control the flow of information, allowing the network to retain long-term dependencies and forget irrelevant past information. This makes LSTMs especially suitable for tasks where long-range temporal dependencies are important, such as detecting collisions in dashcam footage where the current frame might be influenced by the context of previous frames. In collision detection, the model might need to remember several frames of data to predict whether a collision occurs. Frame-by-frame movements or behaviors of objects, such as the change in position of a vehicle, are better captured using sequential models like LSTM. The LSTM model used the following features as inputs: average bounding box area, the change in average bounding box area, the minimum distance to the closest object to the dashcam car, the change in the minimum distance to the closest object in the frame, the area of the largest bounding box, the change in area of the largest bounding box, and the count of objects within a designated “danger zone”.\n\n\nGated Recurrent Unit:\nGRUs are a simpler alternative to LSTMs, designed to achieve similar performance while having fewer parameters. Unlike LSTMs, which use three gates, GRUs rely on only two gates: the reset gate and the update gate. This simpler architecture makes GRUs computationally more efficient, often resulting in faster training times and lower memory usage. Despite their simplicity, GRUs can perform comparably to LSTMs in many sequence modeling tasks, especially when the sequences are of moderate length or when the task does not demand long-term memory. Additionally, GRUs are less prone to overfitting on smaller datasets due to their reduced complexity, making them an ideal choice in scenarios where interpretability and training speed are important.\nTo identify the optimal configuration for both the LSTM and GRU models, we performed a grid search over a defined hyperparameter space including hidden_size, num_layers, dropout, learning_rate, and optimizer type. Each combination of hyperparameters was used to initialize and train an instance of the models. During training, early stopping based on validation loss was employed using a patience parameter to prevent overfitting. Both the LSTM and GRU models use Binary Cross Entropy (BCE) as the loss function because BCE encourages the model to output probabilities near 1 for positive examples and near 0 for negative ones. The training and validation losses were tracked for each configuration, and the model achieving the lowest final validation loss was selected as the best. This thorough approach helped identify the best-performing model configuration for our video classification task.\n\n\nTransformer:\nTransformer models, first introduced in 2017 by Vaswani et al, take a different approach to sequence modeling by replacing recurrence with self-attention mechanisms. Transformers excel in handling input sequence lengths of different sizes. They are also capable of parallelizing computation across multiple positions, and can extract rich contextual information without running into the vanishing-gradient issues that are common to RNNs. Transformers are notoriously difficult to train, so we knew that having an environment that could facilitate high-speed training was important. With this in mind, we decided to host our transformer on a LambdaLabs instance with a Nvidia A10 GPU to increase training efficiency and allow for quicker model iterations.\nFor the goal of detecting collisions through dashcam footage, we built out a classification pipeline using a transformer paired with a BCE loss function to capture the temporal components of our video frame inputs. The process begins by projecting each frame’s raw feature vector into a fixed 128-dimensional embedding space via a linear layer. After this, we apply sinusoidal positional encodings into our feature vector to try and bring back the sequential structure that underlies our raw video data. A LayerNorm preprocessor then stabilizes the embeddings before they are fed into a two layer encoder layer - where each layer uses multi headed attention, and a 512-dimensional feed-forward network using a dropout rate of 0.1 to prevent overfitting. After the encoder, we apply another LayerNorm to prepare our sequence for global aggregation. In this step, we add mask padded positions, sum the amount of valid token embeddings, and divide the sequence lengths to create a single 128-dimensional summary vector per video. For the final step, this vector gets fed into a MLP. For this architecture, we prepare two hidden layers [256, 64], each paired with BatchNorm, ReLU activation, and 0.1 dropout. The output layer consists of two nodes, using sigmoidal activation to yield our final binary classification vector.\n\n\n\n\n Back to top"
  }
]