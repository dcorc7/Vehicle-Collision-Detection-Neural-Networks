---
title: Methods
---

We implemented three neural network architectures for binary classification: LSTM, GRU, and Transformer models. All three models are designed to capture temporal dependencies within video frame sequences by processing frame-level features sequentially. Each model takes a sequence of frame features as input, with dimensions corresponding to the number of frames (time steps) and feature size per frame. 

# Long Short-Term Memory:

LSTM networks are a type of recurrent neural network (RNN) designed to address the problem of vanishing gradients, which can occur when training standard RNNs over long sequences. LSTMs use gates (input, forget, and output) to control the flow of information, allowing the network to retain long-term dependencies and forget irrelevant past information. This makes LSTMs especially suitable for tasks where long-range temporal dependencies are important, such as detecting collisions in dashcam footage where the current frame might be influenced by the context of previous frames. In collision detection, the model might need to remember several frames of data to predict whether a collision occurs. Frame-by-frame movements or behaviors of objects, such as the change in position of a vehicle, are better captured using sequential models like LSTM. The LSTM model used the following features as inputs: average bounding box area, the change in average bounding box area, the minimum distance to the closest object to the dashcam car, the change in the minimum distance to the closest object in the frame, the area of the largest bounding box, the change in area of the largest bounding box, and the count of objects within a designated “danger zone”.

# Gated Recurrent Unit:

GRUs are a simpler alternative to LSTMs, designed to achieve similar performance while having fewer parameters. Unlike LSTMs, which use three gates, GRUs rely on only two gates: the reset gate and the update gate. This simpler architecture makes GRUs computationally more efficient, often resulting in faster training times and lower memory usage. Despite their simplicity, GRUs can perform comparably to LSTMs in many sequence modeling tasks, especially when the sequences are of moderate length or when the task does not demand long-term memory. Additionally, GRUs are less prone to overfitting on smaller datasets due to their reduced complexity, making them an ideal choice in scenarios where interpretability and training speed are important.

To identify the optimal configuration for both the LSTM and GRU models, we performed a grid search over a defined hyperparameter space including hidden_size, num_layers, dropout, learning_rate, and optimizer type. Each combination of hyperparameters was used to initialize and train an instance of the models. During training, early stopping based on validation loss was employed using a patience parameter to prevent overfitting. Both the LSTM and GRU models use Binary Cross Entropy (BCE) as the loss function because BCE encourages the model to output probabilities near 1 for positive examples and near 0 for negative ones. The training and validation losses were tracked for each configuration, and the model achieving the lowest final validation loss was selected as the best. This thorough approach helped identify the best-performing model configuration for our video classification task. 

# Transformer: 

Transformer models, first introduced in 2017 by Vaswani et al, take a different approach to sequence modeling by replacing recurrence with self-attention mechanisms. Transformers excel in handling input sequence lengths of different sizes. They are also capable of parallelizing computation across multiple positions, and can extract rich contextual information without running into the vanishing-gradient issues that are common to RNNs. Transformers are notoriously difficult to train, so we knew that having an environment that could facilitate high-speed training was important. With this in mind, we decided to host our transformer on a LambdaLabs instance with a Nvidia A10 GPU to increase training efficiency and allow for quicker model iterations. 

For the goal of detecting collisions through dashcam footage, we built out a classification pipeline using a transformer paired with a BCE loss function to capture the temporal components of our video frame inputs. The process begins by projecting each frame’s raw feature vector into a fixed 128-dimensional embedding space via a linear layer. After this, we apply sinusoidal positional encodings into our feature vector to try and bring back the sequential structure that underlies our raw video data. A LayerNorm preprocessor then stabilizes the embeddings before they are fed into a two layer encoder layer - where each layer uses multi headed attention, and a 512-dimensional feed-forward network using a dropout rate of 0.1 to prevent overfitting. After the encoder, we apply another LayerNorm to prepare our sequence for global aggregation. In this step, we add mask padded positions, sum the amount of valid token embeddings, and divide the sequence lengths to create a single 128-dimensional summary vector per video. For the final step, this vector gets fed into a MLP. For this architecture, we prepare two hidden layers [256, 64], each paired with BatchNorm, ReLU activation, and 0.1 dropout. The output layer consists of two nodes, using sigmoidal activation to yield our final binary classification vector. 
