---
title: Data & Feature Engineering
---

# Dataset:

This study utilized 1,500 labeled dashcam videos sourced from the Nexar Dashcam Crash Prediction Challenge on Kaggle. Each video clip is roughly 40 seconds and categorized into one of two classes: collision/near miss or non-collision, with an equal distribution of 750 videos per class. The dataset reflects real-world driving conditions and includes a diverse range of environments and traffic scenarios, making it suitable for training temporal models for collision detection. The dataset was partitioned into training, validation, and testing sets using an 80/10/10 split. This ensures sufficient data for model training while reserving representative samples for hyperparameter tuning and final performance evaluation.

The raw dataset can be downloaded [here](https://drive.google.com/uc?export=download&id=1zeR0lXLYOGEjFYbgRmbWjxHL_BmUr2ZZ){download}
 

# Preprocessing:

To convert raw video data into a format suitable for sequence modeling, we performed several preprocessing steps. First, frames were extracted at a rate of 1 frame per second using OpenCV, resulting in a relatively consistent 40 frames per video. Each frame was then passed through a pre-trained YOLOv8 object detection model to identify and localize entities of interest at every time step/frame. The YOLOv8 model is configured to detect around 80 object classes, including many that would appear in dashcam footage, like cars, buses, trucks, pedestrians, and road signs. For each video, a structured “detections.csv” file was generated, logging detected objects and their characteristics. The raw features appearing in each “detections.csv” file include frame number, bounding box size variables (x_min, y_min, x_max, y_max), class, and confidence score. 

From these raw detections, we constructed and appended a suite of per-frame feature vectors by first aggregating and normalizing all of the object-level data over each frame. For each frame, we grouped detections by class and computed their summary statistics, including the count of classes, their mean & variance, and the mean area and aspect ratio of each class’s bounding box. Next, we linked objects across consecutive frames by constructing an IoU-based tracker to estimate each object’s centroid’s displacements, which we use as a proxy for speed and acceleration of different objects over time. To encode short-term temporal context, we also append delta-features (i.e. the frame-to-frame changes in object coins and motion metrics). Finally, all features were standardized using the global mean and standard deviation computed across the training set. All sequences include zero-padding to ensure a uniform length across inputs before feeding into our models. 

The processed frames and extracted detections csv files can be downloaded [here](https://drive.google.com/uc?export=download&id=1c_S2MOFZySFJPpd8GTKxSXKv7SVbG0ad){download}

# Final Input Format:

The final model input consists of a sequence of feature vectors derived from the object detection outputs for each frame. Each sequence has a fixed length of 40 time steps (one per second), with features aggregated or encoded per frame. Sequences are padded and packed to ensure compatibility with PyTorch’s LSTM and GRU modules for batch processing. Each feature vector includes both raw object-level attributes and engineered temporal features that capture inter-frame changes, like shifts in position and relative size. To standardize the input across videos, we used StandardScaler from scikit-learn. This scaler was fit using all feature vectors extracted from the training set. For each video, features were computed frame by frame, and deltas were calculated for a subset of the features by comparing each frame with its preceding frame, while categorical or static features were excluded from delta computation. The final input representation for each frame combined the raw and delta features into a vector, which was then normalized using the fitted scaler before being passed to the models. The dimension of each final input vector varied, as feature importances were being experimented with. The input vectors contained 7, 16, and 28 features for the LSTM, GRU, and Transformer models, respectively.
